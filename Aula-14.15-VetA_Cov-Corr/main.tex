\documentclass{article}
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[vmargin=3cm]{geometry}
\usepackage{tikzpagenodes}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{background}
\usepackage{titlesec}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{hyphenat}
\usepackage[normalem]{ulem}
\usepackage{subcaption}
\hyphenation{mate-mática recu-perar}

\titlespacing{\section}{0pc}{-0.25em}{0pc}
\titlespacing{\subsection}{0pc}{0em}{0pc}
\titlespacing{\subsubsection}{0pc}{0.33em}{0pc}
\titlespacing{\paragraph}{0em}{0.25em}{0.5em}
\setlength{\parindent}{2em}
\setlength{\parskip}{1em}
\linespread{1}

\renewcommand{\baselinestretch}{1.0}

\renewcommand\bf[1]{\textbf{#1}}
\renewcommand\it[1]{\textit{#1}}

\newcommand\ov[1]{\overline{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\vn}{\varnothing}
\newcommand\stk[2][black]{\setbox0=\hbox{$#2$}%
\rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{1pt}}}}#2}

\makeatletter
\global\let\tikz@ensure@dollar@catcode=\relax
\makeatother

\makeatletter
\def\mcolor#1#{\@mcolor{#1}}
\def\@mcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother
\definecolor{notepadrule}{RGB}{217,244,244}

\backgroundsetup{
contents={%
  \begin{tikzpicture}
    \foreach \fila in {0,...,52}
    {
      \draw [line width=1pt,color=notepadrule]
      (current page.west|-0,-\fila*12pt) -- ++(\paperwidth,0);
    }
    \draw[overlay,red!70!black,line width=1pt]
      ([xshift=-1pt]current page text area.west|-current page.north) --
      ([xshift=-1pt]current page text area.west|-current page.south);
  \end{tikzpicture}%
},
scale=1,
angle=0,
opacity=1
}

\begin{document}

\setlength{\abovedisplayskip}{12pt}
\setlength{\belowdisplayskip}{12pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
% \setlength{\baselineskip}{12pt}
\setlength{\jot}{1pt}

\section{Covariância \& Correlação}
Seja $X \in S \to \mathbb{R}^N$ um vetor aleatório e sejam $i,j \in \{1,2,3,\ldots,N\}$ com $i \neq
j$. A covariância entre as componentes $i$ e $j$ é definida por
\begin{align*}
    Cov(X_i,X_j) = \int^{\infty}_{-\infty} \int^{\infty}_{-\infty} (x_i-\mu_i)\cdot(x_j-\mu_j)^* \cdot f_{X_i X_j}(x_i,x_j) dx_i dx_j
\end{align*}
onde $f_{X_i X_j}(x_i,x_j)$ é a PDF marginal das duas componentes e $\mu_i, \mu_j$ são as
expectâncias respectivas das 2 componentes.

A covariância visa observar como o comportamento estatístico de uma componente $X_i$ afeta o
comportamento de $X_j$. Uma covariância positiva significa que o aumento de uma das variáveis em
relação a sua média individual \textit{aumenta} a expectativa de que a outra variável também
cresça em relação a sua média individual. Uma covariância negativa significa que o aumento
\textit{aumenta} a expectativa de uma redução em outra variável. Finalmente, uma covariância nula
mostra que o aumento de uma variável em relação a sua média não altera a expectativa de que outra
variável aumente ou diminua. \textbf{Mas pode haver outro tipo de dependência!}

\textbf{Observação:} $X_1, X_2$ independentes $\to$ covariância nula. $X_1, X_2$ com covariância
nula não implica em independência!
\vspace{0.25em}
\subsection{Estimando Covariância}
Considere que temos $M$ realizações de $X_i, X_j$. A partir dessas realizações temos um estimador
(unbiased) da covariância.

O estimador pode ser tratado como uma nova variável aleatória na qual o valor esperado coincide com
o valor teórico da covariância. \textit{Estamos aproximando a covariância com uma margem de erro!
(Dica de leitura [livro]: Kay, Statistical Signal Processing)}
\begin{align*}
    \ov{Cov_{X_i,X_j}} &= \frac{1}{M-1} \sum_{n=1}^M
    (x_i^{(n)}-\ov{\mu_i})\cdot(x_j^{(n)}\ov{\mu_j})^* \\
    \text{com } \ov{\mu_i} &= \sum_{n=1}^M x_i^{(n)} \qquad \ov{\mu_j} = \sum_{n=1}^M x_j^{(n)}
\end{align*}

\paragraph*{Definição:}
$X_i$ e $X_j$ são não-correlatos se e somente se $Cov_{X_i,X_j} = 0$. \textit{Não confundir com
correlação!}

\section{Correlação}
Correlação entre $X_i$ e $X_j$ é $R(X_i,X_j)$.
\begin{align*}
    R(X_i,X_j) = \int^{\infty}_{-\infty} \int^{\infty}_{-\infty} x_i \cdot x_j^* \cdot f_{X_i X_j}(x_i,x_j) dx_i dx_j
\end{align*}
O interesse por essa grandeza: $Cov(X_i,X_j) = R(X_i,X_j) - \mu_i \cdot \mu_j^*$
\\
Lembrando que $Var[X_i] = \mathbb{E}[X_i^2] - (\mu_i)^2$. \textbf{Observação:} $R(X_i,X_j) = \mathbb{E}(X_iX_j^*)$
\\
Para $Cov(X_i,X_j) = 0$, a correlação torna-se o produto das
médias.

\section{Matrizes de Covariância e Correlação}
Seja $X \in S \to \mathbb{R}^N$ um vetor aleatório e sejam $X_i, X_j$ duas componentes, podemos
obter a correlação e a covariância:
\begin{align*}
    R(X_i,X_j), \; Cov(X_i,X_j)
\end{align*}

Considerando esses valores para todas combinações de $i, j$ dentro do vetor, podemos calcular
$N^2$ correlações e $N^2$ covariâncias e organizar em duas matrizes:
\begin{align*}
    \mathbf{R_{XX}} &= \text{ Matriz de autocorrelação} \\
    &\mathbb{E}[|X_1|^2] \quad\;\; \mathbb{E}[X_1X_2^*] \qquad\;\; \mathbb{E}[X_1X_3^*] \ldots\;
    \mathbb{E}[X_1X_N^*] \\
    &\mathbb{E}[X_2X_1^*] \quad\; \mathbb{E}[|X_2|^2] \qquad\;\;\; \mathbb{E}[X_2X_3^*] \ldots\;
    \mathbb{E}[X_2X_N^*] \\
    &\mathbb{E}[X_NX_1^*] \quad \mathbb{E}[|X_NX_2|^2] \quad\; \mathbb{E}[X_NX_3^*] \ldots \mathbb{E}[X_NX_N^*]
\end{align*}

\vspace{-2.75em}
\begin{align*}
&\mathbf{C_{XX}} = \text{ Matriz de autocovariância} & & &\\
&\sigma_1^2 \mathbb{E}[(X_1-\mu_1)(X_2-\mu_2)^*] \mathbb{E}[(X_1-\mu_1)(X_3-\mu_3)^*] \ldots
\mathbb{E}[(X_1-\mu_1)(X_N-\mu_N)^*] \\
&\mathbb{E}[(X_2-\mu_2)(X_1-\mu_1)^*] \sigma_2^2 \mathbb{E}[(X_2-\mu_2)(X_3-\mu_3)^*] \ldots
\mathbb{E}[(X_2-\mu_2)(X_N-\mu_N)^*] \\
&\mathbb{E}[(X_N-\mu_N)(X_1-\mu_1)^*] \mathbb{E}[|(X_N-\mu_N)(X_2-\mu_2)|^2]
\mathbb{E}[(X_N-\mu_N)(X_3-\mu_3)^*] \ldots \sigma_N^2
\end{align*}

\vspace{-0.75em}
\bf{Observação:} para matriz de autocovariância
     Simétrica no caso real (parte imaginária núla)
     \\
     Hermitiana sempre (é igual a Hermitiana dela própria)
     \\
     Positiva semidefinida \it{(positive semidefinite)} "$\to n \ge 0$" (autovalores reais
        não-negativos)
        \\
     É positiva definida desde que não haja componentes com variância nula (autovalores são
        reais positivos)

\subsection{Notação Matricial}
Para matriz de correlação: $R_{XX} = \mathbb{E}[X \cdot X^H]$, valor esperado de uma matriz aleatória.

Para matriz de covariância:
\begin{align*}
    C_{XX} &= \mathbb{E}[(X-\mu_X)(X-\mu_X)^H] \\
    C_{XX} &= \mathbb{E}[(X-\mu_X)(X^H-\mu_X^H)] \\
    C_{XX} &= \mathbb{E}[XX^H - X\mu_X^H - \mu_XX^H + \mu_X\mu_X^H] \\
    C_{XX} &= R_{XX} - \mathbb{E}[X\mu_X^H] - \mathbb{E}[\mu_XX^H] + \mu_X\mu_X^H \\
    C_{XX} &= R_{XX} - \mathbb{E}[X]\mu_X^H - \mu_X\mathbb{E}[X^H] + \mu_X\mu_X^H \\
    C_{XX} &= R_{XX} - \mu_X\mu_X^H - \mu_X\mu_X^H + \mu_X\mu_X^H \\
    C_{XX} &= R_{XX} - \mu_X\mu_X^H \qquad \text{ uma generalização de: } Var[X] =
    \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\[-2.5em]
\end{align*}
\subsection{Estimador da matriz de covariância}
Considere um vetor aleatório $X \in S \to \mathbb{R}^N$. Suponha que temos $M$ realizações de $X$
sendo: $X^{(1)}, X^{(2)}, X^{(3)} \ldots, X^{(N)}$. Como estimar a matriz de covariância de $C_{XX}$?
\\
Defina uma matriz de dados $D_{[N \times M]}$ em que cada coluna é uma realização $X^{(i)}$. Com
isso, estimamos o vetor média $\mu_X$ somando ao longo das linhas e dividindo por $M$. Realizamos
a subtração de $D$ pelo valor médio correspondente a cada coluna obtendo $D_{\mu}$.
\\
Para obter a estimativa $\ov{C_{XX}}$, fazemos $\ov{C_{XX}}_{[N \times N]} = D_{\mu} D_{\mu}^H$.

\end{document}
